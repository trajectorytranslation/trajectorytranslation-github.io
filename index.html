<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization."
    />
    <meta name="keywords" content="Nerfies, D-NeRF, NeRF" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Abstract-to-Executable Trajectory Translation for One-Shot Task
      Generalization
    </title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap"
      rel="stylesheet"
    />
    <link href="./public/index.css" rel="stylesheet" />
    <link href="./public/media.css" rel="stylesheet" />
  </head>

  <body>
    <main class="content">
      <section class="heading">
        <h1 class="title">
          Abstract-to-Executable Trajectory Translation for One-Shot Task
          Generalization
        </h1>
        <section class="authors">
          <ul>
            <li>
              <span
                ><a
                  href="https://stoneztao.com"
                  rel="noreferrer"
                  target="_blank"
                  >Stone Tao</a
                ></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://sites.google.com/view/xiaochen-li"
                  rel="noreferrer"
                  target="_blank"
                  >Xiaochen Li</a
                ></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://cseweb.ucsd.edu//~t3mu/"
                  rel="noreferrer"
                  target="_blank"
                  >Tongzhou Mu</a
                ></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://sites.google.com/view/zhiao-huang"
                  rel="noreferrer"
                  target="_blank"
                  >Zhiao Huang</a
                ></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://yzqin.github.io/"
                  rel="noreferrer"
                  target="_blank"
                  >Yuzhe Qin</a
                ></span
              >
            </li>
            <li>
              <span
                ><a
                  href="https://cseweb.ucsd.edu/~haosu/"
                  rel="noreferrer"
                  target="_blank"
                  >Hao Su</a
                ></span
              >
            </li>
          </ul>
        </section>
        <section class="affiliations">
          <ul>
            <li>UC San Diego</li>
          </ul>
        </section>
        <section class="links">
          <ul>
            <a href="#" rel="noreferrer" target="_blank">
              <li>
                <span class="icon"> <img src="./public/paper.svg" /> </span
                ><span>Paper</span>
              </li>
            </a>
            <a
              href="https://www.youtube.com/watch?v=M1NA5j6DWHs"
              rel="noreferrer"
              target="_blank"
            >
              <li>
                <span class="icon"> <img src="./public/video.svg" /> </span
                ><span>Video</span>
              </li>
            </a>
            <a
              href="https://github.com/trajectorytranslation/trajectorytranslation"
              rel="noreferrer"
              target="_blank"
            >
              <li>
                <span class="icon">
                  <img src="./public/github.svg" />
                </span>
                <span>Code</span>
              </li>
            </a>
            <!-- <a><li>Video</li></a> -->
          </ul>
        </section>
        <h2>Abstract</h2>
        <p class="abstract">
          Training long-horizon robotic policies in complex physical
          environments is essential for many applications, such as robotic
          manipulation. However, learning a policy that can generalize to unseen
          tasks is challenging. In this work, we propose to achieve one-shot
          task generalization by decoupling plan generation and plan execution.
          Specifically, our method solves complex long-horizon tasks in three
          steps: build a paired abstract environment by simplifying geometry and
          physics, generate abstract trajectories, and solve the original task
          by an abstract-to-executable trajectory translator. In the abstract
          environment, complex dynamics such as physical manipulation are
          removed, making abstract trajectories easier to generate. However,
          this introduces a large domain gap between abstract trajectories and
          the actual executed trajectories as abstract trajectories lack
          low-level details and aren't aligned frame-to-frame with the executed
          trajectory. In a manner reminiscent of language translation, our
          approach leverages a seq-to-seq model to overcome the large domain gap
          between the abstract and executable trajectories, enabling the
          low-level policy to follow the abstract trajectory. Experimental
          results on various unseen long-horizon tasks with different robot
          embodiments demonstrate the practicability of our methods to achieve
          one-shot task generalization.
        </p>
      </section>

      <section class="head-media">
        <video autoplay="" muted="" loop="" height="100%">
          <source
            src="./public/videos/castle-stack-medium.mp4"
            type="video/mp4"
          />
        </video>
        <video autoplay="" muted="" loop="" height="100%">
          <source src="./public/videos/creeperonsnow.mp4" type="video/mp4" />
        </video>
        <video autoplay="" muted="" loop="" height="100%">
          <source src="./public/videos/snowgolem.mp4" type="video/mp4" />
        </video>
        <video autoplay="" muted="" loop="" height="100%">
          <source src="./public/videos/torch.mp4" type="video/mp4" />
        </video>
        <br />
        <p class="caption">
          Videos of succesful long-horizon block stacking tasks performed in the
          real-world using a trajectory translation policy trained in
          simulation. Tasks are unseen and require manipulation of blocks in
          locations beyond the original training distribution.
        </p>
      </section>

      <section class="details">
        <h2>Trajectory Translation (TR<sup>2</sup>)</h2>
        <!-- <p>
          The core idea of our work is to simplify task-solving by enabling the
          low-level agent to explicitly focus only on low-level skills such as
          object manipulation and leave the high-level planning to be solved by
          a high-level agent. The objective of the low-level agent now is to
          simply follow the high-level agent as closely as possible.
        </p> -->
        <p>
          Prior approaches have developed various one-shot imitation learning
          methods that seek to train a policy conditioned on a given
          demonstration/trajectory. We define the source of the trajectory to be
          the high-level agent. By training a policy to follow the trajectory,
          there is great potential for task-generalization as we can simply
          change the trajectory as necessary. However, in prior works oftentimes
          these trajectories are difficult to generate. If the trajectories are
          human video demonstrations, then this becomes infeasible for difficult
          long-horizon tasks as it takes time to generate the videos and some
          tasks may be unsafe for humans. If the trajectories are low-level
          video demonstrations of another robot this is also infeasible as it
          assumes another robot can succesfully solve a task already.
          Furthermore, one cannot easily recover from mistakes via re-planning
          (re-generating trajectories) due to the low-level nature of the
          trajectory or the requirement of human videos.
        </p>
        <p>
          Thus, we seek to simplify this problem by improving the scalability
          and feasibility by utilizing simple high-level agents that generate
          <strong>abstract trajectories</strong>. Abstract trajectories only
          encode simple information about the task at hand. Concretely, in our
          environments our abstract trajectories simply record the 2D/3D
          position of objects in the scene over time, tasking the low-level
          agent to attempt to manipulate the world in a similar manner to
          achieve a desired task. These high-level agents are pointmasses that
          can easily move around in space and magically grasp objects, making
          abstract trajectory generation simple and scalable.
        </p>
        <p>
          Now the problem remains is bridging the domain gap between the
          abstract trajectory and the actual executed trajectory, which may not
          have frame-to-frame alignment. We tackle this problem by utilizing
          seq-to-seq models, specifically transformers, which free us from the
          restriction of frame-to-frame alignment and brdige the domain gap.
        </p>
        <!-- <p>Thus our contributions are three folded: We provide a practical solution </p> -->
        <p>
          The use of abstract trajectories enables flexible definition of novel
          tasks by writing a simple high-level agent to move objects around in
          space. The transformer architecture enables us to more easily follow
          the abstract trajectory as closely as possible. The combination of
          abstract trajectories and transformers enables
          <strong>TR<sup>2</sup></strong> to solve unseen long-horizon tasks. By
          evaluating our method on a navigation-based task and three
          manipulation tasks, we find that our agent achieves strong one-shot
          generalization to new tasks, while being robust to intentional
          interventions or mistakes via re-planning.
        </p>
        <br />
        <img
          style="width: 100%"
          src="./public/translation_transformer_architecture.png"
        />
        <p>
          Above shows an illustration of the abstract-to-executable trajectory
          translation architecture.
          <span style="color: blue">High-level states</span> are fed through one
          encoder and the
          <span style="color: orange">low-level states</span> are fed through a
          separate encoder to create tokens. The tokens form a sequence that
          is given to the transformer model, and the final output embedding
          z<sub>n+k-1</sub> is passed through an MLP to produce
          <span style="color: red">actions</span>.
        </p>
        <h2>Results</h2>
        <p>
          We show example translations of abstract trajectories executed
          trajectories below as well as detail the environments used and domain
          gaps bridged. The left column of videos shows the abstract trajectory
          and the right column shows the executed trajectory. The high-level agents are written using simple heuristics and are represented as a point mass floating in 2D/3D space.
          <!-- Note that while objects like blocks and drawers are rendered in the abstract trajectory display, the abstract trajectory itself only contains 3D position information. -->
        </p>
        
        <div class="abstractexecutable">
          <p>
            Show abstract-to-executable translations on <select>
              <option>Train Tasks</option>
              <option>Test Tasks</option>
            </select>
          </p>
          <div>
            <video autoplay="" muted="" loop="" height="100%">
              <source
                src="./public/videos/box_high_level.mp4"
                type="video/mp4"
              />
            </video>
            <video autoplay="" muted="" loop="" height="100%">
              <source
                src="./public/videos/box_low_level.mp4"
                type="video/mp4"
              />
            </video>
            <p>
              <strong>Box Pusher</strong> <br />
              The training task is to control an agent (black box) to move a
              green box to a target (blue sphere). The high-level agent can
              magically grasp and thus drag the green box. However, the
              low-level agent is restricted to only pushing and must process the abstract trajectory to determine which direction to push the green box in. At test time there
              are obstacles observable only by the high-level agent.
            </p>
          </div>
          <div>
            <video autoplay="" muted="" loop="" height="100%">
              <source
                src="./public/videos/couch_moving_high_level.mp4"
                type="video/mp4"
              />
            </video>
            <video autoplay="" muted="" loop="" height="100%">
              <source
                src="./public/videos/couch_moving_low_level.mp4"
                type="video/mp4"
              />
            </video>
            <p>
              <strong>Couch Moving</strong> <br />
              The training task is to move the couch shaped agent through a map
              of chambers and corners. The agent's couch morphology means that
              the agent must rotate in chambers ahead of time in order to go
              through corners. The high-level agent simply tells the low-level
              agent the path through the map, indicating where corners are, but
              it is up to the low-level agent to process this information to
              determine when to rotate in chambers. At test time, maps are
              longer and vary more.
            </p>
          </div>
          <div>
            <video autoplay="" muted="" loop="" height="100%">
              <source src="./public/videos/abstract_1.mp4" type="video/mp4" />
            </video>
            <video autoplay="" muted="" loop="" height="100%">
              <source src="./public/videos/executed_1.mp4" type="video/mp4" />
            </video>
            <p>
              <strong>Block Stacking</strong> <br />
              The training task is to stack a block with a robot arm. The
              high-level agent can magically grasp and release blocks anywhere
              and move easily through space. The low-level agent must process
              the abstract trajectory to determine where to pick up the block
              and where to stack it. At test time an agent has to stack multiple
              blocks in a row in locations beyond the training distribution.
            </p>
          </div>
          <div>
            <video autoplay="" muted="" loop="" height="100%">
              <source src="./public/videos/drawer_high_level.mp4" type="video/mp4" />
            </video>
            <video autoplay="" muted="" loop="" height="100%">
              <source src="./public/videos/drawer_low_level.mp4" type="video/mp4" />
            </video>
            <p>
              <strong>Open Drawer</strong> <br />
              The training task is open various drawers on cabinets with a
              mobile robot arm. The high-level agent can magically grasp and
              pull open drawers easily. The low-level agent must process the
              abstract trajectory to determine how to follow the abstract
              trajectory and how to pull open the drawer. At test time the agent must open unseen drawers with unseen handles as well as open more than one drawer on a cabinet.
            </p>
          </div>
        </div>
      </section>
      <section class="citation">
        <h2>Bibtex</h2>
        <pre><code>@inproceedings{tao2022tr2,
  title     = {Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization}, 
  author    = {Tao, Stone and Li, Xiaochen and Mu, Tongzhou and Huang, Zhiao and Qin, Yuzhe and Su, Hao},
  year      = {2022},
}</code></pre>
      </section>
      <section class="acknowledgements">
        <h2>Acknowledgements</h2>
        <p>Special thanks to Jiayuan Gu for feedback on figures, and additional members of the SU Lab for writing feedback.</p>
      </section>
    </main>
  </body>
</html>
